import csv
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np
import os
import time

# model
df = pd.read_csv(r'dataset.csv')
print(df.head)
# to print numbers of 0's and1's
print((df.label == 1).sum())
print((df.label == 0).sum())
from collections import Counter


# Count unique words
def counter_word(text_col):
    count = Counter()
    for perm in text_col.values:
        for word in perm.split():
            count[word] += 1
    return count


counter = counter_word(df.perm)
num_unique_words = len(counter)
# training all data
train_size = int(df.shape[0])
train_df = df[:train_size]
train_sentences = train_df.perm.to_numpy()
train_labels = train_df.label.to_numpy()

# Tokenize
from tensorflow.keras.preprocessing.text import Tokenizer

# vectorize a text corpus by turning each text into a sequence of integers
tokenizer = Tokenizer(num_words=num_unique_words)
tokenizer.fit_on_texts(train_sentences)  # fit only to training

# each word has unique index
word_index = tokenizer.word_index
train_sequences = tokenizer.texts_to_sequences(train_sentences)
# Pad the sequences to have the same length
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Max number of words in a sequence
max_length = 50
train_padded = pad_sequences(train_sequences, maxlen=max_length, padding="post", truncating="post")

# Create LSTM model
from tensorflow.keras import layers

# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings
# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)

# Word embeddings give us a way to use an efficient, dense representation in which similar words have
# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a
# dense vector of floating point values (the length of the vector is a parameter you specify).

model = keras.models.Sequential()
model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))

# The layer will take as input an integer matrix of size (batch, input_length),
# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).
# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.

model.add(layers.LSTM(64, dropout=0.1))
model.add(layers.Dense(1, activation="sigmoid"))
model.summary()

loss = keras.losses.BinaryCrossentropy(from_logits=False)
optim = keras.optimizers.Adam(lr=0.001)
metrics = ["accuracy"]
model.compile(loss=loss, optimizer=optim, metrics=metrics)

model.fit(train_padded, train_labels, epochs=15)


# prediction
def test():
    col_names = ['perm']
    df1 = pd.read_csv(r'Responds_from_APP.csv', names=col_names)
    req_size = int(df1.shape[1])
    eva_df = df1[:req_size]
    eva_sentences = eva_df.perm.to_numpy()
    eva_sequences = tokenizer.texts_to_sequences(eva_sentences)
    eva_padded = pad_sequences(eva_sequences, maxlen=max_length, padding="post", truncating="post")
    predictions = model.predict(eva_padded)
    predictions = [1 if p > 0.5 else 0 for p in predictions]
    print(eva_sentences)
    print(predictions)
    return predictions


# # ######################################
# this is code for FLASK server to receives permission from the application
import flask
from flask import Flask, redirect, url_for, request
app = flask.Flask(__name__)
@app.route('/', methods=['GET', 'POST'])
def Request_Respond():
    # "value" variable to receives permissions from the Application
    # and write it in "Responds_from_APP.csv" file
    value = request.form['n1']
    with open('Responds_from_APP.csv', 'w+', encoding='UTF8') as f:
        writer = csv.writer(f)
        writer.writerow([value])
    print(value)
    # we can ignore the prediction of permissions if the "Value" variable was empty
    # which means the application doesn't have permissions
    if value != "":
        # if "value" variable has been received permissions from application, now we can predict
        # if this permission is malicious or normal. then return the result to the application using FLASK API
        Result_of_perdiction = test()
        print(Result_of_perdiction)
        return str(Result_of_perdiction)
    return str(0)
app.run(host="0.0.0.0", port=5000, debug=True)

# the below code for dataset collection from android "AndroidManifest.xml"
#
# import xml.etree.ElementTree as ET
#
# root = ET.parse("AndroidManifest.xml").getroot()
# permissions = root.findall("uses-permission")
# for perm in permissions:
#     for att in perm.attrib:
#         s="{}${}".format(att, perm.attrib[att])
#         split_string = s.split("permission.", 1)
#         substring = split_string[1]
#         print(substring)


###################
